{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importer les Librairies\n"
      ],
      "metadata": {
        "id": "RBxaxrtOLL9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader,Subset,TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "njGKhJlILYOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connexion aux GPUs"
      ],
      "metadata": {
        "id": "-R5YsPdfL64O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PyOJiIM0L9t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connexion à Google Drive"
      ],
      "metadata": {
        "id": "yu_yCDuBMDiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "chemin_images = '/content/drive/MyDrive/Image Datasets' #Choisissez ici le chemin du fichier contenant vos images dans votre Google Drive"
      ],
      "metadata": {
        "id": "8shusSszMItA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importer les Images\n"
      ],
      "metadata": {
        "id": "JKrs39A9MfJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([ T.ToTensor(),\n",
        "                        T.Resize(64,antialias=True),\n",
        "                        T.Normalize([.5,.5,.5],[.5,.5,.5])\n",
        "                       ])\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(root=chemin_images, transform=transform)\n",
        "\n",
        "#Réduction de la taille du dataset si besoin (Pour libérer de la place dans le CPU)\n",
        "# n = 3000\n",
        "# dataset = Subset(dataset,range(n))\n",
        "\n",
        "\n",
        "batchsize   = 64\n",
        "data_loader = DataLoader(dataset,batch_size=batchsize,shuffle=True,drop_last=True)"
      ],
      "metadata": {
        "id": "kdOxc7yRMh9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualiser quelques Images"
      ],
      "metadata": {
        "id": "jWtVDR4sO-PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = next(iter(data_loader))\n",
        "fig,axs = plt.subplots(4,4,figsize=(8,8))\n",
        "for (i,ax) in enumerate(axs.flatten()):\n",
        "  pic = X.data[i].numpy().transpose((1,2,0))\n",
        "  pic = pic/2 + .5\n",
        "  ax.imshow(pic)\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GV4bAsVrPCoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création du Réseau Discriminateur"
      ],
      "metadata": {
        "id": "aX_ervGiNWMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class discriminatorNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # convolution layers\n",
        "    self.conv1 = nn.Conv2d(  3, 64, 4, 2, 1, bias=False)\n",
        "    self.conv2 = nn.Conv2d( 64,128, 4, 2, 1, bias=False)\n",
        "    self.conv3 = nn.Conv2d(128,256, 4, 2, 1, bias=False)\n",
        "    self.conv4 = nn.Conv2d(256,512, 4, 2, 1, bias=False)\n",
        "    self.conv5 = nn.Conv2d(512,  1, 4, 1, 0, bias=False)\n",
        "\n",
        "    # batchnorm\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "    self.bn3 = nn.BatchNorm2d(256)\n",
        "    self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.leaky_relu( self.conv1(x) ,.2)\n",
        "    x = F.leaky_relu( self.conv2(x) ,.2)\n",
        "    x = self.bn2(x)\n",
        "    x = F.leaky_relu( self.conv3(x) ,.2)\n",
        "    x = self.bn3(x)\n",
        "    x = F.leaky_relu( self.conv4(x) ,.2)\n",
        "    x = self.bn4(x)\n",
        "    return torch.sigmoid( self.conv5(x) ).view(-1,1)\n",
        "\n",
        "\n",
        "dnet = discriminatorNet()\n",
        "y = dnet(torch.randn(10,3,64,64))\n",
        "y.shape"
      ],
      "metadata": {
        "id": "HDCUypK-NY9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création du Réseau Générateur"
      ],
      "metadata": {
        "id": "Rm12gm1dNZjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class generatorNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # convolution layers\n",
        "    self.conv1 = nn.ConvTranspose2d(100,512, 4, 1, 0, bias=False)\n",
        "    self.conv2 = nn.ConvTranspose2d(512,256, 4, 2, 1, bias=False)\n",
        "    self.conv3 = nn.ConvTranspose2d(256,128, 4, 2, 1, bias=False)\n",
        "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)\n",
        "    self.conv5 = nn.ConvTranspose2d(64,   3, 4, 2, 1, bias=False)\n",
        "\n",
        "    # batchnorm\n",
        "    self.bn1 = nn.BatchNorm2d(512)\n",
        "    self.bn2 = nn.BatchNorm2d(256)\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "    self.bn4 = nn.BatchNorm2d( 64)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu( self.bn1(self.conv1(x)) )\n",
        "    x = F.relu( self.bn2(self.conv2(x)) )\n",
        "    x = F.relu( self.bn3(self.conv3(x)) )\n",
        "    x = F.relu( self.bn4(self.conv4(x)) )\n",
        "    x = torch.tanh( self.conv5(x) )\n",
        "    return x\n",
        "\n",
        "\n",
        "gnet = generatorNet()\n",
        "y = gnet(torch.randn(10,100,1,1))\n",
        "print(y.shape)\n",
        "pic = y[0,:,:,:].squeeze().detach().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "plt.imshow(pic);"
      ],
      "metadata": {
        "id": "YTTgX6CfNb0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation de modèles déjà entraînés"
      ],
      "metadata": {
        "id": "AsaGxyIXRNBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/Models GAN/\" #Spécifier ici l'endroit où se situe vos fichiers contenant vos modèles sur Google Drive\n",
        "os.makedirs(path, exist_ok=True)\n",
        "model_path_gnet = os.path.join(path, \"model_parameters_gnet_2.pth\")\n",
        "model_path_dnet = os.path.join(path, \"model_parameters_dnet_2.pth\")\n",
        "\n",
        "dnet = discriminatorNet()\n",
        "gnet = generatorNet()\n",
        "\n",
        "# Si Utilisation avec GPU\n",
        "dnet.load_state_dict(torch.load(model_path_dnet))\n",
        "gnet.load_state_dict(torch.load(model_path_gnet))\n",
        "\n",
        "# Si Utilisation sur le CPU de votre PC\n",
        "# dnet.load_state_dict(torch.load(model_path_dnet,map_location=torch.device('cpu')))\n",
        "# gnet.load_state_dict(torch.load(model_path_gnet,map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "dnet = dnet.to(device)\n",
        "gnet = gnet.to(device)\n",
        "\n",
        "lossfun = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0002, betas=(.5,.999))\n",
        "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0002, betas=(.5,.999))\n",
        "\n",
        "len(data_loader)"
      ],
      "metadata": {
        "id": "rgCaR_8eRR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création de nouveaux Modèles"
      ],
      "metadata": {
        "id": "Wtm4q9z6SAi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lossfun = nn.BCELoss()\n",
        "\n",
        "dnet = discriminatorNet().to(device)\n",
        "gnet = generatorNet().to(device)\n",
        "\n",
        "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0002, betas=(.5,.999))\n",
        "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0002, betas=(.5,.999))\n",
        "\n",
        "len(data_loader)"
      ],
      "metadata": {
        "id": "zhaYwS2sSEpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînement des modèles"
      ],
      "metadata": {
        "id": "GydwLFkvNdin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30 #Choisir un nombre d'épisodes adapté au temps que vous disposez pour éxécuter le programme\n",
        "\n",
        "losses  = []\n",
        "disDecs = []\n",
        "training_images = []\n",
        "\n",
        "dnet.train()\n",
        "gnet.train()\n",
        "\n",
        "for epochi in range(num_epochs):\n",
        "\n",
        "  operations = 0\n",
        "\n",
        "  for data,_ in data_loader:\n",
        "\n",
        "    operations += 1\n",
        "\n",
        "    #Création des vecteurs\n",
        "    data = data.to(device)\n",
        "    real_labels = torch.ones(batchsize,1).to(device)\n",
        "    fake_labels = torch.zeros(batchsize,1).to(device)\n",
        "\n",
        "\n",
        "    #Entrainement du Discriminateur\n",
        "    pred_real   = dnet(data)\n",
        "    d_loss_real = lossfun(pred_real,real_labels)\n",
        "\n",
        "    fake_data   = torch.randn(batchsize,100,1,1).to(device)\n",
        "    fake_images = gnet(fake_data)\n",
        "    pred_fake   = dnet(fake_images)\n",
        "    d_loss_fake = lossfun(pred_fake,fake_labels)\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    d_optimizer.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    #Entrainement du Générateur\n",
        "    fake_images = gnet( torch.randn(batchsize,100,1,1).to(device) )\n",
        "    pred_fake   = dnet(fake_images)\n",
        "\n",
        "    g_loss = lossfun(pred_fake,real_labels)\n",
        "\n",
        "    g_optimizer.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    losses.append([d_loss.item(),g_loss.item()])\n",
        "\n",
        "    d1 = torch.mean((pred_real>.5).float()).detach()\n",
        "    d2 = torch.mean((pred_fake>.5).float()).detach()\n",
        "    disDecs.append([d1,d2])\n",
        "\n",
        "    if operations % 20 == 0:\n",
        "      training_images.append(fake_images[0])\n",
        "\n",
        "  #Message de suivi de l'entraînement\n",
        "  msg = f'Épisode {epochi+1}/{num_epochs}'\n",
        "  sys.stdout.write('\\r' + msg)\n",
        "\n",
        "losses  = np.array(losses)\n",
        "#disDecs = np.array(disDecs)"
      ],
      "metadata": {
        "id": "3CjwtanXPXfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affichage des résultats"
      ],
      "metadata": {
        "id": "-LzuvyCeNhx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(x,k=15):\n",
        "  return np.convolve(x,np.ones(k)/k,mode='same')\n",
        "\n",
        "fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
        "\n",
        "ax[0].plot(smooth(losses[:,0]))\n",
        "ax[0].plot(smooth(losses[:,1]))\n",
        "ax[0].set_xlabel('Batches')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model loss')\n",
        "ax[0].legend(['Discrimator','Generator'])\n",
        "\n",
        "ax[1].plot(losses[::5,0],losses[::5,1],'k.',alpha=.1)\n",
        "ax[1].set_xlabel('Discriminator loss')\n",
        "ax[1].set_ylabel('Generator loss')\n",
        "\n",
        "# ax[2].plot(smooth(disDecs[:,0]))\n",
        "# ax[2].plot(smooth(disDecs[:,1]))\n",
        "# ax[2].set_xlabel('Epochs')\n",
        "# ax[2].set_ylabel('Probablity (\"real\")')\n",
        "# ax[2].set_title('Discriminator output')\n",
        "# ax[2].legend(['Real','Fake'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UdhcJzHyNjvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnet.eval()\n",
        "fake_data = gnet( torch.randn(batchsize,100,1,1).to(device) ).cpu()\n",
        "\n",
        "fig,axs = plt.subplots(3,6,figsize=(12,6))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  pic = fake_data[i,:,].detach().squeeze().numpy().transpose((1,2,0))\n",
        "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "  ax.imshow(pic,cmap='gray')\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZnHX1SkTP_MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sauvegarde des modèles"
      ],
      "metadata": {
        "id": "jdIlr4irNkTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/Models GAN/\"\n",
        "os.makedirs(path, exist_ok=True)\n",
        "model_path_gnet = os.path.join(path, \"model_parameters_gnet_2.1.pth\")\n",
        "model_path_dnet = os.path.join(path, \"model_parameters_dnet_2.1.pth\")\n",
        "torch.save(gnet.state_dict(), model_path_gnet)\n",
        "torch.save(dnet.state_dict(), model_path_dnet)"
      ],
      "metadata": {
        "id": "pnkJPmedNmjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affichage de la progression des modèles durant l'entraînement"
      ],
      "metadata": {
        "id": "QxXtm430NnfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, display\n",
        "\n",
        "plt.ion()\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "image_data = np.random.rand(64,64,3)\n",
        "img = ax.imshow(image_data)\n",
        "\n",
        "for image in training_images:\n",
        "    img_np = image.permute(1,2,0).cpu().detach().numpy()\n",
        "    img_np = (img_np-np.min(img_np)) / (np.max(img_np)-np.min(img_np))\n",
        "    plt.imshow(img_np)\n",
        "    clear_output(wait=True)\n",
        "    display(fig)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N6P74m-KNrOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Félicitations si vous en êtes arrivés jusqu'ici !"
      ],
      "metadata": {
        "id": "lGdcW7QZZGaI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}